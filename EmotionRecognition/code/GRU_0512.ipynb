{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc4bd1e0-98ef-4ed4-acb7-ec44d622566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import signal\n",
    "from scipy.integrate import trapz\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Ellipse\n",
    "import heartpy as hp\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', np.RankWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277521be-671d-4402-9b4e-52dfb21aaaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_performance(history, path='./', filename=''):\n",
    "    epoch = list(range(0,len(history.history['loss'])))\n",
    "    \n",
    "    fig, (his_accuracy, his_loss) = plt.subplots(nrows=2,ncols=1,figsize=(10,8),sharex=True)\n",
    "    his_loss.plot(epoch, history.history['loss'], label='Training loss')\n",
    "    his_loss.plot(epoch, history.history['val_loss'], label='Validation loss')\n",
    "    his_loss.set_xlabel(\"Epochs\", fontsize=14)\n",
    "    his_loss.set_ylabel(\"Loss\", fontsize=14)\n",
    "    his_loss.set_title(\"Loss\", fontsize=14)\n",
    "    his_loss.legend(loc='best')\n",
    "    \n",
    "    his_accuracy.plot(epoch, history.history['acc'], label='Training accuracy')\n",
    "    his_accuracy.plot(epoch, history.history['val_acc'], label='Validation accuracy')\n",
    "    his_accuracy.set_xlabel(\"Epochs\", fontsize=14)\n",
    "    his_accuracy.set_ylabel(\"Accuracy\", fontsize=14)\n",
    "    his_accuracy.set_title(\"Accuracy\", fontsize=14)\n",
    "    his_accuracy.legend(loc='best')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    if filename == '':\n",
    "        plt.savefig(f'{path}/performance.png', bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f'{path}/{filename}.png', bbox_inches='tight')\n",
    "        \n",
    "\n",
    "def preprocessing(PPG_data):\n",
    "    index = 0\n",
    "    x = list(range(0,len(PPG_data[0])))\n",
    "    for PPG in PPG_data:\n",
    "        poly = np.polyfit(x, PPG, deg=50)\n",
    "        polied = np.polyval(poly, x)\n",
    "        detrended = PPG - polied\n",
    "        PPG_data[index] = detrended\n",
    "        index += 1\n",
    "    return PPG_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc59573a-f2ec-400e-8175-4cece7c678c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list_ = os.listdir(\"./data\")\n",
    "dir_list_.sort()\n",
    "count = 0\n",
    "dir_list = []\n",
    "# dir_list_.pop(0)\n",
    "dir_list = dir_list_\n",
    "\n",
    "df_PPG = pd.DataFrame(columns=['participant','video_number','ppg','valence','arousal'])\n",
    "\n",
    "file_num = 0\n",
    "for file in dir_list:\n",
    "    dat_file = f'./data/{file}'\n",
    "    with open(dat_file, 'rb') as f:\n",
    "        Channel_data = pickle.load(f,encoding='latin1')\n",
    "    data = Channel_data[\"data\"]\n",
    "    labels = Channel_data[\"labels\"]\n",
    "    for video_num in range(40):\n",
    "        dataP = data[video_num,38]\n",
    "        dataP = dataP[384:]\n",
    "        \n",
    "        label = labels[video_num]\n",
    "        if label[0] < 5:\n",
    "            Valence = 0\n",
    "        if label[0] >= 5:\n",
    "            Valence = 1\n",
    "        if label[1] < 5:\n",
    "            Arousal = 0\n",
    "        if label[1] >= 5:\n",
    "            Arousal = 1\n",
    "        instance = [(file_num, video_num, dataP, Valence, Arousal)]\n",
    "        new_instance = pd.DataFrame(instance, columns=['participant','video_number','ppg','valence','arousal'])\n",
    "        df_PPG = pd.concat([df_PPG, new_instance], ignore_index = True)\n",
    "    \n",
    "    file_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14222127-a03e-497e-92cd-4eef7a2d2f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPG_data = df_PPG['ppg']\n",
    "Valence = df_PPG['valence']\n",
    "Arousal = df_PPG['arousal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc4370e1-8143-4f84-9621-0537411cb879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sselab/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/numpy/lib/polynomial.py:627: RuntimeWarning: overflow encountered in multiply\n",
      "  scale = NX.sqrt((lhs*lhs).sum(axis=0))\n",
      "/home/sselab/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/numpy/core/_methods.py:47: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    }
   ],
   "source": [
    "preprocessed = preprocessing(PPG_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b46a6ea-39b5-451d-93cf-99b55a62c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_video = 12\n",
    "preprocessed = preprocessed[:-40*num_video]\n",
    "Valence = Valence[:-40*num_video]\n",
    "Arousal = Arousal[:-40*num_video]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e68f3924-34b9-4e75-a625-c6ceeda90753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "# window = 1280\n",
    "timestep = 1280\n",
    "segmented = []\n",
    "valence_data = []\n",
    "arousal_data = []\n",
    "\n",
    "################################################\n",
    "# for index in range(len(preprocessed) - timestep): \n",
    "#     segmented.append(preprocessed[index: index + timestep])\n",
    "idx = 0\n",
    "for PPG in preprocessed:\n",
    "    for index in range(0, len(PPG) - timestep, 128):\n",
    "        segmented.append(PPG[index : index + timestep])\n",
    "        valence_data.append(Valence[idx])\n",
    "        arousal_data.append(Arousal[idx])\n",
    "    idx += 1\n",
    "print(len(segmented))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27780a13-df4b-4058-8567-8179fb169177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "# from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout,BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bd2f6d2-a7c6-4590-945b-ef1b99fd8cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 2 GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=6000)]) # limit in megabytes\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcc4e0fc-e9a7-4ae9-a7c9-3ce9320ec36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = segmented\n",
    "train_valence = valence_data\n",
    "train_arousal = arousal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "963005d0-d0ab-4fce-8c09-4e142c64720e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 1280) (40000,)\n",
      "X_train shape : (40000, 1280, 1), Y_train shape : (40000, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train_data = np.array(train_x)\n",
    "y_train_data = np.array(train_valence).astype(np.float32)\n",
    "print(x_train_data.shape, y_train_data.shape)\n",
    "\n",
    "x_train_data = x_train_data.reshape((x_train_data.shape[0],x_train_data.shape[1], 1))\n",
    "\n",
    "# normalize train data\n",
    "nsamples, nx, ny = x_train_data.shape\n",
    "train_dataset = x_train_data.reshape((nsamples,nx*ny))\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_train_data = scaler.fit_transform(train_dataset)\n",
    "a = 1000\n",
    "x_train_data = x_train_data * a\n",
    "\n",
    "X_train = x_train_data.reshape((x_train_data.shape[0], nx, ny))\n",
    "# X_train = x_train_data.reshape((x_train_data.shape[0], x_train_data.shape[1],1))\n",
    "Y_train = y_train_data.reshape((y_train_data.shape[0],1))\n",
    "\n",
    "print(f\"X_train shape : {X_train.shape}, Y_train shape : {Y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fbe6e36-8e35-4e91-ad0e-f7f105e829cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters of the model\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c05959cd-4f97-4392-bbee-460aae5598e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 256)               198912    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 462,081\n",
      "Trainable params: 462,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "\n",
    "\n",
    "# one GRU layers with three dense layers-> IT WORKS !\n",
    "model = Sequential()\n",
    "model.add(GRU(units=256,\n",
    "              dropout=0.25,\n",
    "              recurrent_dropout=0.25,\n",
    "              return_sequences=False,\n",
    "              input_shape=X_train[0].shape))\n",
    "\n",
    "# Dense net\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "callback_list = [tf.keras.callbacks.ModelCheckpoint(filepath='BinaryClassification_Dense3-callback.h5', monitor='val_accuracy', save_best_only=True),\n",
    "                tf.keras.callbacks.EarlyStopping(patience=30)]\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate), metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e4dee8a-9f6b-44f3-a3cb-0a32b9d6c16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.7610 - acc: 0.5312WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "250/250 [==============================] - 2573s 10s/step - loss: 0.7610 - acc: 0.5312 - val_loss: 0.6834 - val_acc: 0.5875\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6906 - acc: 0.5389WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "250/250 [==============================] - 2609s 10s/step - loss: 0.6906 - acc: 0.5389 - val_loss: 0.6837 - val_acc: 0.5875\n",
      "Epoch 3/100\n",
      "213/250 [========================>.....] - ETA: 5:42 - loss: 0.6904 - acc: 0.5382"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:66\u001b[0m, in \u001b[0;36menable_multi_worker.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_method_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     65\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_multi_worker_mode():  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m   \u001b[38;5;66;03m# Running inside `run_distribute_coordinator` already.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dc_context\u001b[38;5;241m.\u001b[39mget_current_worker_context():\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:848\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m traceme\u001b[38;5;241m.\u001b[39mTraceMe(\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraceContext\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    843\u001b[0m     graph_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    844\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m    845\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m    846\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size):\n\u001b[1;32m    847\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 848\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m   \u001b[38;5;66;03m# Catch OutOfRangeError for Datasets of unknown size.\u001b[39;00m\n\u001b[1;32m    850\u001b[0m   \u001b[38;5;66;03m# This blocks until the batch has finished executing.\u001b[39;00m\n\u001b[1;32m    851\u001b[0m   \u001b[38;5;66;03m# TODO(b/150292341): Allow multiple async steps here.\u001b[39;00m\n\u001b[1;32m    852\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39minferred_steps:\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_function\u001b[39m(iterator):\n\u001b[1;32m    570\u001b[0m   data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m--> 571\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m    574\u001b[0m       outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    575\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m    947\u001b[0m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[1;32m    948\u001b[0m   \u001b[38;5;66;03m# applied when when the caller is also in Eager mode.\u001b[39;00m\n\u001b[1;32m    949\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m    950\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 951\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2288\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2289\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 2290\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m   2646\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\n\u001b[1;32m   2647\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy(),\n\u001b[1;32m   2648\u001b[0m       replica_id_in_sync_group\u001b[38;5;241m=\u001b[39mconstant_op\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;241m0\u001b[39m, dtypes\u001b[38;5;241m.\u001b[39mint32)):\n\u001b[0;32m-> 2649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:282\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    281\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:531\u001b[0m, in \u001b[0;36mModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    528\u001b[0m x, y, sample_weight \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39munpack_x_y_sample_weight(data)\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backprop\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m--> 531\u001b[0m   y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m   loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled_loss(\n\u001b[1;32m    533\u001b[0m       y, y_pred, sample_weight, regularization_losses\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# For custom training steps, users can just write:\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m#   trainable_variables = self.trainable_variables\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m#   gradients = tape.gradient(loss, trainable_variables)\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m#   self.optimizer.apply_gradients(zip(gradients, trainable_variables))\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m# The _minimize call does a few extra steps unnecessary in most cases,\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# such as loss scaling and gradient clipping.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:968\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    965\u001b[0m cast_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m base_layer_utils\u001b[38;5;241m.\u001b[39mautocast_context_manager(\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype):\n\u001b[0;32m--> 968\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_mask_metadata(inputs, outputs, input_masks)\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:277\u001b[0m, in \u001b[0;36mSequential.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    275\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_graph_network(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 277\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSequential\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m outputs \u001b[38;5;241m=\u001b[39m inputs  \u001b[38;5;66;03m# handle the corner case where self.layers is empty\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    281\u001b[0m   \u001b[38;5;66;03m# During each iteration, `inputs` are the inputs to `layer`, and `outputs`\u001b[39;00m\n\u001b[1;32m    282\u001b[0m   \u001b[38;5;66;03m# are the outputs of `layer` applied to `inputs`. At the end of each\u001b[39;00m\n\u001b[1;32m    283\u001b[0m   \u001b[38;5;66;03m# iteration `inputs` is set to `outputs` to prepare for the next layer.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:717\u001b[0m, in \u001b[0;36mNetwork.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_graph_network:\n\u001b[1;32m    714\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen subclassing the `Model` class, you should\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    715\u001b[0m                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m implement a `call` method.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_internal_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_kwargs_to_constants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_layer_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaving\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:888\u001b[0m, in \u001b[0;36mNetwork._run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    885\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(_map_tensor_if_from_keras_layer, kwargs)\n\u001b[1;32m    887\u001b[0m \u001b[38;5;66;03m# Compute outputs.\u001b[39;00m\n\u001b[0;32m--> 888\u001b[0m output_tensors \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomputed_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    892\u001b[0m     nest\u001b[38;5;241m.\u001b[39mflatten(node\u001b[38;5;241m.\u001b[39moutput_tensors), nest\u001b[38;5;241m.\u001b[39mflatten(output_tensors)):\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py:654\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m inputs, initial_state, constants \u001b[38;5;241m=\u001b[39m _standardize_args(inputs,\n\u001b[1;32m    649\u001b[0m                                                      initial_state,\n\u001b[1;32m    650\u001b[0m                                                      constants,\n\u001b[1;32m    651\u001b[0m                                                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_constants)\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m constants \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 654\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# If any of `initial_state` or `constants` are specified and are Keras\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# tensors, then add them to the inputs and temporarily modify the\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# input_spec to include them.\u001b[39;00m\n\u001b[1;32m    660\u001b[0m additional_inputs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:968\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    965\u001b[0m cast_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m base_layer_utils\u001b[38;5;241m.\u001b[39mautocast_context_manager(\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype):\n\u001b[0;32m--> 968\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_mask_metadata(inputs, outputs, input_masks)\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py:423\u001b[0m, in \u001b[0;36mGRU.call\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(cell_inputs, cell_states):\n\u001b[1;32m    421\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell(cell_inputs, cell_states, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 423\u001b[0m last_output, outputs, states \u001b[38;5;241m=\u001b[39m \u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgo_backwards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgo_backwards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43munroll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munroll\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_lengths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow_lengths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_major\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_major\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mzero_output_for_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_output_for_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# This is a dummy tensor for testing purpose.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m runtime \u001b[38;5;241m=\u001b[39m _runtime(_RUNTIME_UNKNOWN)\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4236\u001b[0m, in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   4233\u001b[0m     new_states \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(initial_states, flat_new_state)\n\u001b[1;32m   4234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (time \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, output_ta_t) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mtuple\u001b[39m(new_states)\n\u001b[0;32m-> 4236\u001b[0m   final_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcontrol_flow_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4237\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4238\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_ta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4239\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mwhile_loop_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4240\u001b[0m   new_states \u001b[38;5;241m=\u001b[39m final_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m   4242\u001b[0m output_ta \u001b[38;5;241m=\u001b[39m final_outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py:2727\u001b[0m, in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2724\u001b[0m loop_var_structure \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(type_spec\u001b[38;5;241m.\u001b[39mtype_spec_from_value,\n\u001b[1;32m   2725\u001b[0m                                         \u001b[38;5;28mlist\u001b[39m(loop_vars))\n\u001b[1;32m   2726\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cond(\u001b[38;5;241m*\u001b[39mloop_vars):\n\u001b[0;32m-> 2727\u001b[0m   loop_vars \u001b[38;5;241m=\u001b[39m \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloop_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2728\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m try_to_pack \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loop_vars, (\u001b[38;5;28mlist\u001b[39m, _basetuple)):\n\u001b[1;32m   2729\u001b[0m     packed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4222\u001b[0m, in \u001b[0;36mrnn.<locals>._step\u001b[0;34m(time, output_ta_t, *states)\u001b[0m\n\u001b[1;32m   4220\u001b[0m current_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(ta\u001b[38;5;241m.\u001b[39mread(time) \u001b[38;5;28;01mfor\u001b[39;00m ta \u001b[38;5;129;01min\u001b[39;00m input_ta)\n\u001b[1;32m   4221\u001b[0m current_input \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(inputs, current_input)\n\u001b[0;32m-> 4222\u001b[0m output, new_states \u001b[38;5;241m=\u001b[39m \u001b[43mstep_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4223\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4224\u001b[0m flat_state \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mflatten(states)\n\u001b[1;32m   4225\u001b[0m flat_new_state \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mflatten(new_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py:421\u001b[0m, in \u001b[0;36mGRU.call.<locals>.step\u001b[0;34m(cell_inputs, cell_states)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(cell_inputs, cell_states):\n\u001b[0;32m--> 421\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:968\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    965\u001b[0m cast_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m base_layer_utils\u001b[38;5;241m.\u001b[39mautocast_context_manager(\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype):\n\u001b[0;32m--> 968\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_mask_metadata(inputs, outputs, input_masks)\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py:1821\u001b[0m, in \u001b[0;36mGRUCell.call\u001b[0;34m(self, inputs, states, training)\u001b[0m\n\u001b[1;32m   1818\u001b[0m   h_tm1_r \u001b[38;5;241m=\u001b[39m h_tm1\n\u001b[1;32m   1819\u001b[0m   h_tm1_h \u001b[38;5;241m=\u001b[39m h_tm1\n\u001b[0;32m-> 1821\u001b[0m recurrent_z \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mdot(h_tm1_z, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecurrent_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m   1822\u001b[0m recurrent_r \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mdot(h_tm1_r,\n\u001b[1;32m   1823\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurrent_kernel[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m   1824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_after \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1224\u001b[0m, in \u001b[0;36m_SliceHelperVar\u001b[0;34m(var, slice_spec)\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_SliceHelperVar\u001b[39m(var, slice_spec):\n\u001b[1;32m   1182\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a slice helper object given a variable.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \n\u001b[1;32m   1184\u001b[0m \u001b[38;5;124;03m  This allows creating a sub-tensor from part of the current contents\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \n\u001b[1;32m   1222\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1224\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _slice_helper(\u001b[43mvar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, slice_spec, var)\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:550\u001b[0m, in \u001b[0;36mBaseResourceVariable.value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_value\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28;01mNone\u001b[39;00m, ignore_existing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 550\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_variable_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:645\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    643\u001b[0m       result \u001b[38;5;241m=\u001b[39m read_and_set_handle()\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 645\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mread_and_set_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m    648\u001b[0m   \u001b[38;5;66;03m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[1;32m    649\u001b[0m   \u001b[38;5;66;03m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[1;32m    650\u001b[0m   tape\u001b[38;5;241m.\u001b[39mrecord_operation(\n\u001b[1;32m    651\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadVariableOp\u001b[39m\u001b[38;5;124m\"\u001b[39m, [result], [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle],\n\u001b[1;32m    652\u001b[0m       backward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x],\n\u001b[1;32m    653\u001b[0m       forward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:635\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[0;34m()\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_and_set_handle\u001b[39m():\n\u001b[0;32m--> 635\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mgen_resource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_variable_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m   _maybe_set_handle_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle, result)\n\u001b[1;32m    638\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/EmotionRecognition/lib/python3.8/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py:468\u001b[0m, in \u001b[0;36mread_variable_op\u001b[0;34m(resource, dtype, name)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m    467\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReadVariableOp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop_callbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m    472\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,\n",
    "                  Y_train,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  validation_split=0.2,\n",
    "                  callbacks=callback_list,\n",
    "                  shuffle=True,\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0268f77e-5156-4f51-8b58-10dbdc51339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_performance(history,path='./plot_performance',filename='GRU_performance3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce28bb-ec12-46bc-8f10-fe581cc94cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
