{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, layers, callbacks\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.core.pylabtools import figsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of features data : 840\n",
      "length of Arousal labels data : 840 length of Valence labels data : 840\n"
     ]
    }
   ],
   "source": [
    "# for natural sorting\n",
    "import re\n",
    "\n",
    "def tryint(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except:\n",
    "        return s\n",
    "def alphanum_key(s):\n",
    "    \"\"\" Turn a string into a list of string and number chunks.\n",
    "        \"z23a\" -> [\"z\", 23, \"a\"]\n",
    "    \"\"\"\n",
    "    return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
    "def sort_nicely(l):\n",
    "    \"\"\" Sort the given list in the way that humans expect.\n",
    "    \"\"\"\n",
    "    l.sort(key=alphanum_key)\n",
    "    \n",
    "\n",
    "# load features data\n",
    "dir_path = \"/tmp/elias/emotion_recognition/features\"\n",
    "\n",
    "dir_list = os.listdir(dir_path)\n",
    "dir_list.sort()\n",
    "dir_list.pop(0)\n",
    "sort_nicely(dir_list)\n",
    "\n",
    "path = \"/tmp/elias/emotion_recognition/features/\"\n",
    "features_data = []\n",
    "for file in dir_list:\n",
    "    data = np.loadtxt(path+file, delimiter=',',skiprows=1, usecols=range(1,19))\n",
    "    features_data.append(data)\n",
    "\n",
    "print(\"length of features data :\", len(features_data))    \n",
    "\n",
    "\n",
    "# load labels data    \n",
    "dir_list_ = os.listdir(\"/tmp/elias/emotion_recognition/data\")\n",
    "dir_list_.sort()\n",
    "dir_list_ = dir_list_[:21]\n",
    "\n",
    "labels_Valence = []\n",
    "labels_Arousal = []\n",
    "\n",
    "for file in dir_list_:\n",
    "    dat_file = '/tmp/elias/emotion_recognition/data/' + file\n",
    "    with open(dat_file, 'rb') as f:\n",
    "        Channel_data =pickle.load(f,encoding='latin1')\n",
    "    labels = Channel_data[\"labels\"]\n",
    "    for value in labels:\n",
    "        if value[0] < 5:\n",
    "            labels_Valence.append(0)\n",
    "        if value[0] >= 5:\n",
    "            labels_Valence.append(1)\n",
    "        if value[1] < 5:\n",
    "            labels_Arousal.append(0)\n",
    "        if value[1] >= 5:\n",
    "            labels_Arousal.append(1)\n",
    "\n",
    "print(\"length of Arousal labels data :\", len(labels_Arousal), \"length of Valence labels data :\", len(labels_Valence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0vid_1_1.csv\n",
      "[[104.91666667 106.5          0.         ...   0.         100.\n",
      "    0.        ]\n",
      " [104.91666667 106.5          0.         ...   0.         100.\n",
      "    0.        ]\n",
      " [104.91666667 106.5          0.         ...   0.         100.\n",
      "    0.        ]\n",
      " ...\n",
      " [105.90909091 114.           1.         ...   0.         100.\n",
      "    0.        ]\n",
      " [105.90909091 114.           1.         ...   0.         100.\n",
      "    0.        ]\n",
      " [105.90909091 114.           1.         ...   0.         100.\n",
      "    0.        ]]\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# make sure you loaded data with no error\n",
    "print(dir_list[0])\n",
    "print(features_data[0])\n",
    "print(labels_Arousal[0])\n",
    "print(labels_Valence[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# data = pd.read_csv('/tmp/elias/emotion_recognition/features/0vid_1_1.csv', header = None)\n",
    "# import csv\n",
    "# file = open(\"/tmp/elias/emotion_recognition/features/0vid_1_1.csv\", \"r\")\n",
    "# csv_reader = csv.reader(file)\n",
    "\n",
    "# lists_from_csv = []\n",
    "# for row in csv_reader:\n",
    "#     lists_from_csv.append(row)\n",
    "# lists_from_csv.pop(0)\n",
    "# print(lists_from_csv)\n",
    "# print(len(lists_from_csv[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-05 02:24:47.039618: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2022-02-05 02:24:47.085224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2022-02-05 02:24:47.085653: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-02-05 02:24:47.089054: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2022-02-05 02:24:47.091324: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2022-02-05 02:24:47.091716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2022-02-05 02:24:47.094436: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-02-05 02:24:47.095973: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-02-05 02:24:47.101699: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-02-05 02:24:47.103080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
      "2022-02-05 02:24:47.103945: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2022-02-05 02:24:47.132233: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2099990000 Hz\n",
      "2022-02-05 02:24:47.134519: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c4b0461730 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-02-05 02:24:47.134549: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-02-05 02:24:47.135357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2022-02-05 02:24:47.135404: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-02-05 02:24:47.135431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2022-02-05 02:24:47.135456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2022-02-05 02:24:47.135481: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2022-02-05 02:24:47.135506: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-02-05 02:24:47.135530: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-02-05 02:24:47.135554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-02-05 02:24:47.136849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\n",
      "2022-02-05 02:24:47.136901: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-02-05 02:24:47.343218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-02-05 02:24:47.343253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \n",
      "2022-02-05 02:24:47.343264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \n",
      "2022-02-05 02:24:47.345324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4000 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:02:00.0, compute capability: 7.5)\n",
      "2022-02-05 02:24:47.347916: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c4ad715740 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-02-05 02:24:47.347937: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 2 GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)]) # limit in megabytes\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test\n",
    "- what we have to do for this\n",
    " - 동영상 기준? ㅁ\n",
    " - 샘플 기준?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "168\n"
     ]
    }
   ],
   "source": [
    "# train 80%, test 20%\n",
    "train_data = features_data[:2]\n",
    "test_data = features_data[672:]\n",
    "train_data_Vlabels = labels_Valence[:2]\n",
    "train_data_Alabels = labels_Arousal[:2]\n",
    "test_data_Vlabels = labels_Valence[672:]\n",
    "test_data_Alabels = labels_Arousal[672:]\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[104.91666667 106.5          0.         ...   0.         100.\n",
      "     0.        ]\n",
      "  [104.91666667 106.5          0.         ...   0.         100.\n",
      "     0.        ]\n",
      "  [104.91666667 106.5          0.         ...   0.         100.\n",
      "     0.        ]\n",
      "  ...\n",
      "  [105.90909091 114.           1.         ...   0.         100.\n",
      "     0.        ]\n",
      "  [105.90909091 114.           1.         ...   0.         100.\n",
      "     0.        ]\n",
      "  [105.90909091 114.           1.         ...   0.         100.\n",
      "     0.        ]]\n",
      "\n",
      " [[102.58333333 111.5          0.         ...   0.         100.\n",
      "     0.        ]\n",
      "  [102.58333333 111.5          0.         ...   0.         100.\n",
      "     0.        ]\n",
      "  [102.58333333 111.5          0.         ...   0.         100.\n",
      "     0.        ]\n",
      "  ...\n",
      "  [109.27272727 109.           0.         ...   0.         100.\n",
      "     0.        ]\n",
      "  [109.27272727 109.           0.         ...   0.         100.\n",
      "     0.        ]\n",
      "  [109.27272727 109.           0.         ...   0.         100.\n",
      "     0.        ]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 6400, 18)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_data = np.array(train_data)\n",
    "x_test_data = np.array(test_data)\n",
    "y_train_data = np.array(train_data_Vlabels)\n",
    "x_train_data.shape , y_train_data.shape\n",
    "print(x_train_data)\n",
    "\n",
    "# normalize data\n",
    "nsamples, nx, ny = x_train_data.shape\n",
    "train_dataset = x_train_data.reshape((nsamples,nx*ny))\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_train_data = scaler.fit_transform(train_dataset)\n",
    "\n",
    "###\n",
    "# X_train = x_train_data.reshape((x_train_data.shape[0], x_train_data.shape[1], 18))\n",
    "X_train = x_train_data.reshape((x_train_data.shape[0], 6400, 18))\n",
    "Y_train = y_train_data.reshape((y_train_data.shape[0], 1))\n",
    "# array([[a],[b],[c]...])\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RNN, LSTM, GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters of the model\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400, 18)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "# model.add(GRU(units=64, activation='relu', input_shape=X_train[0].shape))\n",
    "model.add(GRU(units=64, activation='relu', input_shape=(6400,18),return_sequences=True))\n",
    "model.add(GRU(units=128, return_sequences = True))\n",
    "model.add(GRU(units=128, return_sequences =True))\n",
    "model.add(GRU(units=256))\n",
    "\n",
    "## gotta add dropout!\n",
    "# model.add(GRU(units=128, return_sequences=True, return_state=True,dropout=0.2))\n",
    "\n",
    "# Dense net\n",
    "model.add(Dense(256))\n",
    "# model.dropout(0.2)\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=learning_rate),metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "hist=model.fit(X_train,Y_train,epochs=epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 6400, 64)          16128     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 6400, 128)         74496     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 256)               296448    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 494,081\n",
      "Trainable params: 494,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(GRU(units=256, activation='relu', input_shape=(6400,18), return_sequences= True))\n",
    "\n",
    "## gotta add dropout!\n",
    "# model.add(GRU(units=128, return_sequences=True, return_state=True,dropout=0.2))\n",
    "# Dense net\n",
    "model1.add(Dense(256))\n",
    "model1.add(Dense(128))\n",
    "model1.add(Dense(64))\n",
    "model1.add(Dense(1))\n",
    "\n",
    "model1.compile(loss='binary_crossentropy',optimizer=Adam(lr=learning_rate),metrics=['mae'])\n",
    "# model1.compile(loss='binary_crossentropy',optimizer=Adam(lr=learning_rate),metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-05 02:24:51.091524: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2022-02-05 02:24:53.055609: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 15s 7s/sample - loss: 8.9032 - mae: 0.9996\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.6075 - mae: 0.4189\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 0.5211\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 1.5240\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 2.5682\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 3.6372\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 4.7174\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 5.7961\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 6.8631\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 7.9097\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 8.9282\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 9.9136\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 10.8619\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 11.7692\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 12.6332\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 13.4525\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 12s 6s/sample - loss: 0.0000e+00 - mae: 14.2265\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 14.9553\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 15.6393\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 16.2805\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 16.8794\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 17.4374\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 17.9562\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 18.4377\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 18.8837\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 19.2964\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 19.6775\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 20.0291\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 20.3531\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 20.6513\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 20.9255\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 21.1774\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 21.4086\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 12s 6s/sample - loss: 0.0000e+00 - mae: 21.6207\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 21.8150\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 21.9930\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 22.1559\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 12s 6s/sample - loss: 0.0000e+00 - mae: 22.3049\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 22.4411\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 22.5656\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 22.6793\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 22.7830\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 22.8777\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 22.9641\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.0428\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.1146\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.1800\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.2395\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.2938\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.3431\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.3880\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.4289\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.4661\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.4999\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.5306\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.5586\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.5840\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.6070\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 12s 6s/sample - loss: 0.0000e+00 - mae: 23.6280\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.6470\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.6643\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 12s 6s/sample - loss: 0.0000e+00 - mae: 23.6800\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.6943\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.7072\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.7189\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.7296\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.7392\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.7480\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.7559\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.7632\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.7697\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.7756\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 12s 6s/sample - loss: 0.0000e+00 - mae: 23.7810\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.7859\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.7903\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.7943\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.7979\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.8012\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.8042\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.8069\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.8093\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.8115\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.8135\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.8154\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.8170\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.8185\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.8198\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.8211\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 12s 6s/sample - loss: 0.0000e+00 - mae: 23.8222\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.8232\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.8241\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.8249\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.8256\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.8263\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.8269\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.8275\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.8280\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.8284\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 11s 6s/sample - loss: 0.0000e+00 - mae: 23.8288\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 11s 5s/sample - loss: 0.0000e+00 - mae: 23.8292\n"
     ]
    }
   ],
   "source": [
    "hist=model1.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    RepeatVector(X_train.shape[1]), # replicates features from outputs (30 times)\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    # Time distributed layer to get an output with right shape\n",
    "    TimeDistributed(Dense(X_train.shape[2]))\n",
    "])\n",
    "model.compile(loss='binary_clas', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model3 = Sequential([\n",
    "    LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    RepeatVector(X_train.shape[1]), # replicates features from outputs (30 times)\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    # Time distributed layer to get an output with right shape\n",
    "    Dense(64),\n",
    "    Dense(1)\n",
    "])\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_607201/2758143763.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model3' is not defined"
     ]
    }
   ],
   "source": [
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "hist=model3.fit(X_train,Y_train,epochs=epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inputs = keras.Input(shape=(6400,18))\n",
    "model = GRU(units=64,return_sequences=True,activation='relu')(inputs)\n",
    "model = GRU(units=128,return_sequences=True,activation='relu')(model)\n",
    "model = GRU(units=256,return_sequences=True,activation='relu')(model)\n",
    "model = GRU(units=128,activation='relu')(model)\n",
    "model = layers.Dense(64,activation='relu')(model)\n",
    "outputs = layers.Dense(1,activation='sigmoid')(model)\n",
    "\n",
    "model1 = keras.Model(inputs,outputs)\n",
    "model1.summary()\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## bidirectional\n",
    "model1 = Sequential([\n",
    "Bidirectional(layers.LSTM(units=100,activation='relu', return_sequences=True, return_state=True, input_shape=(1, 50))),\n",
    "Bidirectional(layers.LSTM(units=100,activation='relu', return_sequences=True, return_state=True)),\n",
    "Bidirectional(layers.LSTM(units=100,activation='relu', return_sequences=True, return_state=False)),\n",
    "Dense(units=100,activation='relu'),\n",
    "Dropout(0.1),\n",
    "Dense(units=100,activation='relu'),\n",
    "Dense(1)\n",
    "])\n",
    "\n",
    "##\n",
    "# model = Sequential()\n",
    "# # model.add(GRU(units=64, activation='relu', input_shape=X_train[0].shape))\n",
    "# model.add(GRU(units=64, activation='relu', input_shape=(672,18)))\n",
    "# model.add(GRU(units=128, return_sequences = True, return_state = True))\n",
    "# # model.add(GRU(units=128, return_sequences =True, return_state=True, statefull=True))\n",
    "# model.add(GRU(units=128))\n",
    "\n",
    "\n",
    "# ## gotta add dropout!\n",
    "# # model.add(GRU(units=128, return_sequences=True, return_state=True,dropout=0.2))\n",
    "\n",
    "# # Dense net\n",
    "# # model.add(Dense(128))\n",
    "# # model.add(Dense(256))\n",
    "# # model.add(Dense(512))\n",
    "# # model.add(Dense(1024))\n",
    "# # model.add(Dense(512))\n",
    "# model.add(Dense(128))\n",
    "# model.add(Dense(64))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "model1.compile(loss='binary_crossentropy',optimizer=Adam(lr=learning_rate),metrics=['mae'])\n",
    "history = model1.fit(X_train,Y_train, epochs=epochs,validation_split=0.1)\n",
    "model1.evaluate(test_input,test_output)\n",
    "# model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
