{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, layers, callbacks\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.core.pylabtools import figsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# for natural sorting\n",
    "import re\n",
    "\n",
    "def tryint(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except:\n",
    "        return s\n",
    "def alphanum_key(s):\n",
    "    \"\"\" Turn a string into a list of string and number chunks.\n",
    "        \"z23a\" -> [\"z\", 23, \"a\"]\n",
    "    \"\"\"\n",
    "    return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
    "def sort_nicely(l):\n",
    "    \"\"\" Sort the given list in the way that humans expect.\n",
    "    \"\"\"\n",
    "    l.sort(key=alphanum_key)\n",
    "    \n",
    "\n",
    "# load features data\n",
    "dir_path = \"/tmp/elias/emotion_recognition/features\"\n",
    "\n",
    "dir_list = os.listdir(dir_path)\n",
    "dir_list.sort()\n",
    "dir_list.pop(0)\n",
    "sort_nicely(dir_list)\n",
    "\n",
    "dir_list = dir_list[:800]\n",
    "# print(dir_list)\n",
    "print(len(dir_list))\n",
    "\n",
    "path = \"/tmp/elias/emotion_recognition/features/\"\n",
    "features_data = []\n",
    "count = 0\n",
    "for file in dir_list:\n",
    "    data = np.loadtxt(path+file, delimiter=',',skiprows=1, usecols=range(1,13))\n",
    "    features_data.append(data)\n",
    "    print(count)\n",
    "    count +=1\n",
    "\n",
    "print(\"length of features data :\", len(features_data))    \n",
    "\n",
    "\n",
    "# load labels data    \n",
    "dir_list_ = os.listdir(\"/tmp/elias/emotion_recognition/data\")\n",
    "dir_list_.sort()\n",
    "dir_list_ = dir_list_[:20]\n",
    "\n",
    "labels_Valence = []\n",
    "labels_Arousal = []\n",
    "\n",
    "for file in dir_list_:\n",
    "    dat_file = '/tmp/elias/emotion_recognition/data/' + file\n",
    "    with open(dat_file, 'rb') as f:\n",
    "        Channel_data =pickle.load(f,encoding='latin1')\n",
    "    labels = Channel_data[\"labels\"]\n",
    "    for value in labels:\n",
    "        if value[0] < 5:\n",
    "            labels_Valence.append(0)\n",
    "        if value[0] >= 5:\n",
    "            labels_Valence.append(1)\n",
    "        if value[1] < 5:\n",
    "            labels_Arousal.append(0)\n",
    "        if value[1] >= 5:\n",
    "            labels_Arousal.append(1)\n",
    "\n",
    "print(\"length of Arousal labels data :\", len(labels_Arousal), \"length of Valence labels data :\", len(labels_Valence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for natural sorting\n",
    "import re\n",
    "\n",
    "def tryint(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except:\n",
    "        return s\n",
    "def alphanum_key(s):\n",
    "    \"\"\" Turn a string into a list of string and number chunks.\n",
    "        \"z23a\" -> [\"z\", 23, \"a\"]\n",
    "    \"\"\"\n",
    "    return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
    "def sort_nicely(l):\n",
    "    \"\"\" Sort the given list in the way that humans expect.\n",
    "    \"\"\"\n",
    "    l.sort(key=alphanum_key)\n",
    "    \n",
    "\n",
    "# load features data\n",
    "dir_path = \"/tmp/elias/emotion_recognition/features\"\n",
    "\n",
    "dir_list = os.listdir(dir_path)\n",
    "dir_list.sort()\n",
    "dir_list.pop(0)\n",
    "sort_nicely(dir_list)\n",
    "\n",
    "dir_list = dir_list[:800]\n",
    "# print(dir_list)\n",
    "print(len(dir_list))\n",
    "\n",
    "path = \"/tmp/elias/emotion_recognition/features/\"\n",
    "features_data = []\n",
    "count = 0\n",
    "for file in dir_list:\n",
    "    data = np.loadtxt(path+file, delimiter=',',skiprows=1, usecols=range(1,13))\n",
    "    features_data.append(data)\n",
    "#     print(count)\n",
    "    count +=1\n",
    "\n",
    "print(\"length of features data :\", len(features_data))    \n",
    "\n",
    "\n",
    "# load labels data    \n",
    "dir_list_ = os.listdir(\"/tmp/elias/emotion_recognition/data\")\n",
    "dir_list_.sort()\n",
    "dir_list_ = dir_list_[:20]\n",
    "label = []\n",
    "\n",
    "for file in dir_list_:\n",
    "    dat_file = '/tmp/elias/emotion_recognition/data/' + file\n",
    "    with open(dat_file, 'rb') as f:\n",
    "        Channel_data =pickle.load(f,encoding='latin1')\n",
    "    labels = Channel_data[\"labels\"]\n",
    "    for value in labels:\n",
    "        if value[0] < 5:\n",
    "            if value[1] < 5:\n",
    "                label.append(0) # [0 0 0 0] V=0, A=0\n",
    "            else:\n",
    "                label.append(1) # [0 1 0 0] V=0, A=1\n",
    "        else:\n",
    "            if value[1] < 5:\n",
    "                label.append(2) # [0 0 1 0] V=1, A=0\n",
    "            else:\n",
    "                label.append(3) # [0 0 0 0] V=1, A=1\n",
    "\n",
    "# print(\"length of Arousal labels data :\", len(labels_Arousal), \"length of Valence labels data :\", len(labels_Valence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "  \n",
    "     \n",
    "# list of name, degree, score \n",
    "field = [\"aparna\", \"pankaj\", \"sudhir\", \"Geeku\"] \n",
    "deg = [\"MBA\", \"BCA\", \"M.Tech\", \"MBA\"] \n",
    "scr = [90, 40, 80, 98] \n",
    "     \n",
    "# dictionary of lists  \n",
    "dict = {'name': nme, 'degree': deg, 'score': scr}  \n",
    "       \n",
    "df = pd.DataFrame(dict) \n",
    "    \n",
    "# saving the dataframe \n",
    "df.to_csv('GFG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['meanNN', 'medianNN', 'NN50', 'pNN50', 'RMSSD', 'VLF_a', 'LF_a', 'HF_a', 'Total_a', 'VLF_h', 'LF_h', 'HF_h', 'VLF_p', 'LF_p', 'HF_p', 'LF_n', 'HF_n', 'LFHF']\n"
     ]
    }
   ],
   "source": [
    "fname = path + \"0vid_1_1.csv\"\n",
    "a = pd.read_csv(fname)\n",
    "b = a.columns\n",
    "# print(b)\n",
    "b = b.to_list()\n",
    "b.pop(0)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/elias/emotion_recognition'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ABORTED\n",
    "## features data to_one CSV file\n",
    "\n",
    "import csv\n",
    "# field names \n",
    "fields = b \n",
    "    \n",
    "# data rows of csv file \n",
    "rows = features_data\n",
    "\n",
    "path = '/tmp/elias/emotion_recognition/features_csv/features_data.csv'\n",
    "with open(path, 'w') as f:\n",
    "      \n",
    "    # using csv.writer method from CSV package\n",
    "    write = csv.writer(f)\n",
    "      \n",
    "    write.writerow(fields)\n",
    "    write.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# make sure you loaded data with no error\n",
    "print(dir_list[0])\n",
    "print(features_data[0])\n",
    "print(labels_Arousal[0])\n",
    "print(labels_Valence[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# data = pd.read_csv('/tmp/elias/emotion_recognition/features/0vid_1_1.csv', header = None)\n",
    "# import csv\n",
    "# file = open(\"/tmp/elias/emotion_recognition/features/0vid_1_1.csv\", \"r\")\n",
    "# csv_reader = csv.reader(file)\n",
    "\n",
    "# lists_from_csv = []\n",
    "# for row in csv_reader:\n",
    "#     lists_from_csv.append(row)\n",
    "# lists_from_csv.pop(0)\n",
    "# print(lists_from_csv)\n",
    "# print(len(lists_from_csv[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 2 GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=6000)]) # limit in megabytes\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test\n",
    "- what we have to do for this\n",
    " - 동영상 기준? ㅁ\n",
    " - 샘플 기준?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train 80%, test 20%\n",
    "train_data = features_data[:672]\n",
    "test_data = features_data[672:]\n",
    "train_data_Vlabels = labels_Valence[:672]\n",
    "train_data_Alabels = labels_Arousal[:672]\n",
    "test_data_Vlabels = labels_Valence[672:]\n",
    "test_data_Alabels = labels_Arousal[672:]\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 80%, test 20%\n",
    "train_data = features_data[:672]\n",
    "test_data = features_data[672:]\n",
    "train_data_labels = labels[:672]\n",
    "test_data_labels = labels[672:]\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set train and test data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train_data = np.array(train_data)\n",
    "y_train_data = np.array(train_data_Vlabels)\n",
    "x_test_data = np.array(test_data)\n",
    "y_test_data = np.array(test_data_Vlabels)\n",
    "x_train_data.shape , y_train_data.shape\n",
    "\n",
    "# normalize data\n",
    "Train_nsamples, Train_nx, Train_ny = x_train_data.shape\n",
    "train_dataset = x_train_data.reshape((Train_nsamples,Train_nx*Train_ny))\n",
    "Test_nsamples, Test_nx, Test_ny = x_test_data.shape\n",
    "test_dataset = x_test_data.reshape((Test_nsamples,Test_nx*Test_ny))\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_train_data = scaler.fit_transform(train_dataset)\n",
    "x_test_data = scaler.fit_transform(test_dataset)\n",
    "\n",
    "###\n",
    "# X_train = x_train_data.reshape((x_train_data.shape[0], x_train_data.shape[1], 18))\n",
    "X_train = x_train_data.reshape((Train_nsamples, Train_nx, Train_ny))\n",
    "X_test = x_test_data.reshape((Test_nsamples, Test_nx, Test_ny))\n",
    "Y_train = y_train_data.reshape((y_train_data.shape[0], 1))\n",
    "Y_test = y_test_data.reshape((y_test_data.shape[0], 1))\n",
    "# array([[a],[b],[c]...])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data = np.array(train_data)\n",
    "y_train_data = np.array(train_data_labels)\n",
    "x_test_data = np.array(test_data)\n",
    "y_test_data = np.array(test_data_labels)\n",
    "x_train_data.shape , y_train_data.shape\n",
    "\n",
    "# # normalize data\n",
    "# Train_nsamples, Train_nx, Train_ny = x_train_data.shape\n",
    "# train_dataset = x_train_data.reshape((Train_nsamples,Train_nx*Train_ny))\n",
    "# Test_nsamples, Test_nx, Test_ny = x_test_data.shape\n",
    "# test_dataset = x_test_data.reshape((Test_nsamples,Test_nx*Test_ny))\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# x_train_data = scaler.fit_transform(train_dataset)\n",
    "# x_test_data = scaler.fit_transform(test_dataset)\n",
    "\n",
    "# ###\n",
    "# # X_train = x_train_data.reshape((x_train_data.shape[0], x_train_data.shape[1], 18))\n",
    "# X_train = x_train_data.reshape((Train_nsamples, Train_nx, Train_ny))\n",
    "# X_test = x_test_data.reshape((Test_nsamples, Test_nx, Test_ny))\n",
    "# Y_train = y_train_data.reshape((y_train_data.shape[0], 2))\n",
    "# Y_test = y_test_data.reshape((y_test_data.shape[0], 2))\n",
    "# # array([[a],[b],[c]...])\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RNN, LSTM, GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters of the model\n",
    "learning_rate = 0.001\n",
    "epochs = 200\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(128,\n",
    "                             dropout=0.25,\n",
    "                             recurrent_dropout=0.25,\n",
    "                             return_sequences=True,\n",
    "                             input_shape=X_train[0].shape)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(256,\n",
    "                             dropout=0.25,\n",
    "                             recurrent_dropout=0.25,\n",
    "                             return_sequences=False)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=64,\n",
    "#                     validation_split=0.2,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one GRU layers with three dense layers-> IT WORKS !\n",
    "model = Sequential()\n",
    "model.add(GRU(units=128,\n",
    "#               dropout=0.25,\n",
    "#               recurrent_dropout=0.25,\n",
    "              return_sequences=False,\n",
    "              input_shape=X_train[0].shape))\n",
    "\n",
    "## gotta add dropout!\n",
    "\n",
    "# Dense net\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# two GRU layers with four dense layers-> IT WORKS !\n",
    "model = Sequential()\n",
    "model.add(GRU(units=128,\n",
    "              dropout=0.25,\n",
    "              recurrent_dropout=0.25,\n",
    "              return_sequences=True,\n",
    "              input_shape=X_train[0].shape))\n",
    "\n",
    "# model.add(GRU(units=128,\n",
    "#               dropout=0.25,\n",
    "#               recurrent_dropout=0.25,\n",
    "#               return_sequences =True))\n",
    "\n",
    "model.add(GRU(units=256,\n",
    "              return_sequences =False))\n",
    "\n",
    "# Dense net\n",
    "# model.add(Dense(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# two GRU layers with five dense layers-> IT WORKS !\n",
    "model = Sequential()\n",
    "model.add(GRU(units=128,\n",
    "              dropout=0.25,\n",
    "              recurrent_dropout=0.25,\n",
    "              return_sequences=True,\n",
    "              input_shape=X_train[0].shape))\n",
    "\n",
    "model.add(GRU(units=256,\n",
    "              return_sequences =False))\n",
    "\n",
    "## gotta add dropout!\n",
    "\n",
    "# Dense net\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " been# three GRU layers -> It doesn't work...\n",
    "model = Sequential()\n",
    "model.add(GRU(units=64,\n",
    "              dropout=0.25,\n",
    "              recurrent_dropout=0.25,\n",
    "              return_sequences=True,\n",
    "              input_shape=X_train[0].shape))\n",
    "\n",
    "model.add(GRU(units=128,\n",
    "              dropout=0.25,\n",
    "              recurrent_dropout=0.25,\n",
    "              return_sequences =True))\n",
    "\n",
    "model.add(GRU(units=256,\n",
    "              return_sequences =False))\n",
    "\n",
    "## gotta add dropout!\n",
    "\n",
    "# Dense net\n",
    "# model.add(Dense(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " been# two bidirectional GRU layers -> it doesn't work..\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(GRU(units=128,\n",
    "                            dropout=0.25,\n",
    "                            recurrent_dropout=0.25,\n",
    "                            return_sequences=True,\n",
    "                            input_shape=X_train[0].shape)))\n",
    "\n",
    "# model.add(GRU(units=128,\n",
    "#               dropout=0.25,\n",
    "#               recurrent_dropout=0.25,\n",
    "#               return_sequences =True))\n",
    "\n",
    "model.add(Bidirectional(GRU(units=256,\n",
    "                            dropout=0.25,\n",
    "                            recurrent_dropout=0.25,\n",
    "                            return_sequences=False)))\n",
    "\n",
    "## gotta add dropout!\n",
    "\n",
    "# Dense net\n",
    "# model.add(Dense(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(X_train,\n",
    "                  Y_train,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  validation_data=(X_test,Y_test),\n",
    "                  verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model1 = Sequential()\n",
    "# model1.add(GRU(units=64, activation='relu', input_shape=X_train[0].shape))\n",
    "model1.add(GRU(units=64, activation='relu', input_shape=(6400,18),return_sequences=True))\n",
    "# model1.add(GRU(units=128, return_sequences = True))\n",
    "# model1.add(GRU(units=128, return_sequences =True))\n",
    "model1.add(GRU(units=256))\n",
    "\n",
    "## gotta add dropout!\n",
    "# model.add(GRU(units=128, return_sequences=True, return_state=True,dropout=0.2))\n",
    "\n",
    "# Dense net\n",
    "model1.add(Dense(256))\n",
    "# model.dropout(0.2)\n",
    "model1.add(Dense(128))\n",
    "model1.add(Dense(64))\n",
    "model1.add(Dense(1))\n",
    "\n",
    "model1.compile(loss='binary_crossentropy',optimizer=Adam(lr=learning_rate),metrics=['mae'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inputs = keras.Input(shape=(6400,18))\n",
    "model = GRU(units=64,return_sequences=True,activation='relu')(inputs)\n",
    "model = GRU(units=128,return_sequences=True,activation='relu')(model)\n",
    "model = GRU(units=256,return_sequences=True,activation='relu')(model)\n",
    "model = GRU(units=128,activation='relu')(model)\n",
    "model = layers.Dense(64,activation='relu')(model)\n",
    "outputs = layers.Dense(1,activation='sigmoid')(model)\n",
    "\n",
    "model1 = keras.Model(inputs,outputs)\n",
    "model1.summary()\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "hist=model1.fit(X_train,Y_train,epochs=epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## bidirectional\n",
    "model1 = Sequential([\n",
    "Bidirectional(layers.LSTM(units=100,activation='relu', return_sequences=True, return_state=True, input_shape=(1, 50))),\n",
    "Bidirectional(layers.LSTM(units=100,activation='relu', return_sequences=True, return_state=True)),\n",
    "Bidirectional(layers.LSTM(units=100,activation='relu', return_sequences=True, return_state=False)),\n",
    "Dense(units=100,activation='relu'),\n",
    "Dropout(0.1),\n",
    "Dense(units=100,activation='relu'),\n",
    "Dense(1)\n",
    "])\n",
    "\n",
    "##\n",
    "# model = Sequential()\n",
    "# # model.add(GRU(units=64, activation='relu', input_shape=X_train[0].shape))\n",
    "# model.add(GRU(units=64, activation='relu', input_shape=(672,18)))\n",
    "# model.add(GRU(units=128, return_sequences = True, return_state = True))\n",
    "# # model.add(GRU(units=128, return_sequences =True, return_state=True, statefull=True))\n",
    "# model.add(GRU(units=128))\n",
    "\n",
    "\n",
    "# ## gotta add dropout!\n",
    "# # model.add(GRU(units=128, return_sequences=True, return_state=True,dropout=0.2))\n",
    "\n",
    "# # Dense net\n",
    "# # model.add(Dense(128))\n",
    "# # model.add(Dense(256))\n",
    "# # model.add(Dense(512))\n",
    "# # model.add(Dense(1024))\n",
    "# # model.add(Dense(512))\n",
    "# model.add(Dense(128))\n",
    "# model.add(Dense(64))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "model1.compile(loss='binary_crossentropy',optimizer=Adam(lr=learning_rate),metrics=['mae'])\n",
    "history = model1.fit(X_train,Y_train, epochs=epochs,validation_split=0.1)\n",
    "model1.evaluate(test_input,test_output)\n",
    "# model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
